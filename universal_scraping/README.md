#üåå Universal web scraping, filtering and simple content analysis in Python

Questo progetto √® un esperimento/evoluzione del progetto facente parte del capstone alla fine del percorso 'Python for everybody'(Coursera), sono partito dal progetto originale 'pagerank'.
L'ho realizzato prima del termine del corso per consolidare delle nozioni appena acquisite.

##üéØ Obbiettivo del progetto

Il mio scopo era trasformare qualcosa di gi√† pronto in altro che potessi sentire pi√π 'mio', anche se ovviamente mi mancano ancora le conoscenze per coprire alcune parti. Volevo rendere quello strumenti pi√π universale e pi√π comodo da usare.
Nel mio caso ho usato Wikipedia e trovavo fastidioso il fatto che nello scraping avevo molte pagine 'vuote' e non interessanti per la mia ricerca, come collegamanti ed indici.

In pochi punti:
-pi√π flessibiile
-pi√π controllabile
-pi√π dati puliti

##ü§ñ Uso dell'inteligenza artificiale (LLM--> Gemini, ChatGPT)

Durante lo sviluppo, pur portando le mie idee al progetto, ho usato l'AI per aiutarmi, esplorando argomenti che giustamente con un corso base non avevo neanche trattato,(la parte della visualizzazione per quanto riguarda il file excel e html sono state interamente fatte da l'AI senza nessun mio contributo).
Comunque decisioni finali e andamento del progetto hanno la mia paternit√†.

In pochi punti:
-chiarire concetti ancora non consolidati
-esplorare soluzioni alternative
-migliorare la struttura del codice
-comprendere errori e comportamenti inattesi su alcuni siti web

##üöÄ Funzionalit√† principali

###üî∑ Sistema filtri dinamici
(Whitelist/Blacklist)
-menu interattivo da terminale
-visualizzazione dei filtri
-possibilit√† tra:
 -aggiungere parola chiave
 -escludere termini
 -svuotare liste
 -reset filtri
 
###üï∑Ô∏è Spider avanzato
-User-Agent personalizzato per simulare un browser reale
-integrazione filtri direttamente nella fase di scraping
-parsing mirato a contenuti html


###üìä Analisi testuale
-conteggio parole per singolo link
-analisi quantit√† testo pagine

###üìà Esportazione e visualizzazione 
Completamente AI(senza un mio intervento)

##üß† Architettura del progetto

### Ordine di esecuzione consigliata

1_manager.py --> configurazione (whitelist/blacklist)
2_spider.py --> scraping mirato delle pagine
3_rank.py --> ranking delle pagine
4_analyzer --> analisi delle parole
5_visualizzazione1_esportadati.py --> creazione file cvs per excel
6_visualizzazione2_generagrafico.py --> visualizzazione grafica diretta tramite file html

Possono essere eseguiti anche i file originali dopo la parte dello spider per avere una configurazione a nodi.

##üîß Tecnologie utilizzate
-Python
-Web scraping
-SQLite
-HTML

##üìå Note finali

Questo progetto √® stato molto importante perch√® mi ha permesso di scoprire quante funzioni possono essere collegate insieme per progettare un sistema completo e funzionale.

A termine ho poi trovato alcune conclusioni, nello specifico io volevo cercare quale civilt√† storica fosse la pi√π documentata su Wikipedia, ma la ricerca per parole chiavi mi ha portato a pi√π separazioni, come per esempio Regno/Civilt√†/Popolo dividendo di fatto alcuni argomenti appartenenti alla stessa civilt√†.
Una possibile soluzione sarebbe stata l'analisi del titolo, magari analizzando tutti i titoli e con la parola pi√π frequente(es. Roma,romani) avrebbe creato un unico blocco. Oppure usare WikiData sfruttando i codici, ancora meglio per i computer, nonostante le possibilit√† ho preferito fermarmi sia per concludere il corso, una questione di tempo, sia perch√® gi√† per la parte di visualizzazione ho dovuto far fare tutto all'AI e non mi piaceva continuare in quel senso, non sarebbe stato pi√π il 'mio' progetto.